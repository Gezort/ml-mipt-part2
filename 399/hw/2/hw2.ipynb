{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Organization Info</h1> \n",
    "\n",
    "**Дополнительный материал для выполнения дз**:\n",
    "- Hastie, The Elements of Statistical Learning, https://goo.gl/k3wfEU\n",
    "    - 2.9 Model Selection and the Bias–Variance Tradeoff \n",
    "    - 15 Random Forests\n",
    "    - 10 Boosting and Additive Trees 337\n",
    "- Andrew Ng, Bias vs. Variance, https://goo.gl/1ISZ6Y\n",
    "- Соколов, Семинары по композиционным методам, https://goo.gl/sn8RyJ, http://goo.gl/ajNTQy\n",
    "\n",
    "**Оформление дз**: \n",
    "- Необходимо прислать два файла - ipynb и его pdf-версию. Получить pdf-версию можно ”печатью страницы в файл”. Известно, что данная процедура корректно работает в Яндекс.Браузере и часто работает некорректно в Firefox.\n",
    "- Внимательно обратите внимание на полученную pdf-версию. Никакая информация не должна выходить за видимую область. Допускается ”разрезание” некоторой информации на два листа.\n",
    "- Присылайте выполненное задание на почту ``probability.diht@yandex.ru``\n",
    "- Укажите тему письма в следующем формате ``[ML 399] <фамилия> - <что за задание>``, к примеру -- ``[ML 399] Раздолбайников - Заданице 1``\n",
    "\n",
    "--------\n",
    "- **PS1**: Мы используем автоматические фильтры, и просто не найдем ваше дз, если вы не аккуратно его подпишите.\n",
    "- **PS2**: Напоминаем, что дедлайны жесткие, письма пришедшие после автоматически удаляются =( чтобы соблазна не было "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> \n",
    "\n",
    "Ответьте кратко на вопросы своими словами (загугленный материал надо пересказать), ответ обоснуйте (напишите и ОБЪЯСНИТЕ формулки если потребуется). Если что-то не получается, вернитесь к лекции, семинару или дополнительным материалам.\n",
    "\n",
    "**Вопрос 1**: Чем отличается AdaBoost от XGBoost? Перечислите принципиальные отличия. \n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 2**: Почему говорят, что AdaBoost неустойчив к выбросам?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 3**:  В каком пространстве градиентный бустинг совершает градиентный спуск? Какова размерность этого пространства?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 4**: В чем заключается сокращение шага в градиентном бустинге? Как число итераций, необходимое для сходимости, зависит от размера шага η?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 5**: Что такое стохастический градиентный бустинг?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 6**: Какой смысл у шума, смещения, разброса?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 7**: Приведите пример функции, приближающей исходную зависимость, с маленьким смещением и большим разбросом. Приведите пример функции с большим смещением и маленьким разбросом.\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 8**: Как сгенерировать бутстрепную выборку? Какая длина у нее длина?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 9**: Что такое бэггинг?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 10**:  Как соотносятся смещение разброс композиции, построенной с помощью бэггинга, со смещением и разбросом одного базового алгоритма?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 11**: Почему хорошими базовыми алгоритмами для бэггинга являются именно деревья?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "-----------\n",
    "PS: Если проверяющий не понял ответ на большинство вопросов, то будет пичалька. Пишите так, чтобы можно было разобраться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Теоретические задачи</h1> \n",
    "\n",
    "Решение можно написать здесь. А можно и на листочке понятным почерком. В таком случае нужно выслать его скан или принести на семинар.\n",
    "\n",
    "**Задача 1.** Пусть $X_1, ..., X_n$ --- одномерная выборка. Рассмотрим класс функций $\\mathscr{F} = \\{f_{x_0}^{s}(x)\\ \\left| \\ x_0 \\in \\mathbb{R}, s \\in \\{+, -\\} \\right.\\}$, где $f^+_{x_0}(x) = I\\{x > x_0\\}$ и $f^-_{x_0}(x) = I\\{x < x_0\\}$.\n",
    "Покажите, что при любых $Y_1, ..., Y_n$, где $Y_i \\in \\{-1, +1\\}$, существует композиция вида\n",
    "$$F(x) = sign \\sum\\limits_{t=1}^{2n+2} \\alpha_t f_t(x),\\ \\ \\ \\alpha \\in \\mathbb{R},\\ f_t \\in \\mathscr{F},$$\n",
    "не допускающая ошибок на обучающей выборке.\n",
    "\n",
    "**Задача 2.** Найдите градиент логистической функции потерь $\\mathscr{L} (y_1, y_2) = \\log \\left(1 + e^{-y_1 y_2} \\right)$ для фиксированного объекта.\n",
    "\n",
    "**Задача 3.** Известно, что бэггинг плохо работает, если в качестве базовых классификаторов взять kNN. Попробуем понять причины на простом примере.\n",
    "\n",
    "Пусть дана выборка $(X_1, Y_1), ..., (X_n, Y_n)$, причем $Y \\in \\{−1, +1\\}$. Будем рассматривать классификатор одного ближайшего соседа в качестве компоненты смеси. Построим с помощью бэггинга композицию длины $T$:\n",
    "$$F(x) = sign \\sum\\limits_{t=1}^{T} f_t(x) $$\n",
    "Оцените вероятность того, что ответ композиции на произвольном объекте $X$ будет отличаться от ответа одного классификатора ближайшего соседа, обученного по всей выборке. Покажите, что эта вероятность стремится к нулю при $T \\to \\infty$.\n",
    "\n",
    "-----\n",
    "**<Решение>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Binary Boosting Implementation</h1> \n",
    "\n",
    "Нужно реализовать двухклассовый бустинг с логистичиской функцией потерь. \n",
    "\n",
    "Длину шага -- или используйте $1.0*lr$ или подбирайте одномерной оптимизацией;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from utils import plot_surface\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryBoostingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, n_estimators, lr=0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator: sklearn.Classifier\n",
    "            Компонента для смеси, которую можно обучить (есть метод fit).\n",
    "            Для обучение композиции нужно много таких, можно получить с помощю copy.deepcopy\n",
    "\n",
    "        n_estimators: int\n",
    "            Число компонент в композиции\n",
    "\n",
    "        lr: float > 0\n",
    "            Шаг обучения\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lr = lr   \n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "    \n",
    "    def loss_grad(self, original_y, pred_y):\n",
    "        return  # Градиент на кажом объекте\n",
    "        \n",
    "    def fit(self, X, original_y):\n",
    "        \"\"\"\n",
    "        Метод должен обучить композицию, используя X, y как обучающую выборку.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array\n",
    "        y: 1d np.array\n",
    "        \"\"\"\n",
    "        \n",
    "        # Храните компоненты смеси тут\n",
    "        self.estimators_ = [] \n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            grad = self.loss_grad(original_y, self._predict(X))\n",
    "            # Настройте классификатор на антиградиент\n",
    "            estimator = <Тут Ваш код>\n",
    "            self.estimators_.append(estimator)\n",
    "        \n",
    "        self.out_ = self.outliers(grad)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        y_pred = <Получите ответ композиции до применения решающего правила>\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = <примените к self._predict решающее правило>\n",
    "        return y_pred\n",
    "    \n",
    "    def outliers(self, grad, k=10):\n",
    "        return  # Топ-k объектов с большим отрицательным отступом (M = F(X) * Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Simple test</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=2,\n",
    "                           n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2,\n",
    "                           flip_y=0.05, class_sep=0.8, random_state=241)\n",
    "y = 2 * (y - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = BinaryBoostingClassifier(n_estimators=100).fit(X, y)\n",
    "plot_surface(X, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Outliers</h1> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<Нарисуйте только outliers> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Adult test</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sh ./get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\n",
    "    './data/adult.data', \n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"], \n",
    "    header=None, na_values=\"?\")\n",
    "adult = pd.get_dummies(adult)\n",
    "adult[\"Target\"] = adult[\"Target_ >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "y = 2 * (y - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<Сверьте качество своего классификатора с GradientBoostingClassifier>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Text classification</h1> \n",
    "\n",
    "- Найдите двухклассовый текстовый датасет (в качестве примера sentiment analysis) или возьмите многоклассовый и классифцируйте один клас против остальных\n",
    "- Попробуйте бустинг на решающих деревьях, в качестве фичей используйте tf-idf и svd/random_projection/hashing_trick, что работает лучше? Сравните качество и время работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Bagging Implementation</h1> \n",
    "\n",
    "Реализуйте беггинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from scipy.stats import randint\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class BaggingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, n_estimators, items_rate=1.0, features_rate=1.0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator: sklearn.Classifier\n",
    "            Компонента для смеси, которую можно обучить (есть метод fit).\n",
    "            Для обучение композиции нужно много таких, можно получить с помощю copy.deepcopy\n",
    "\n",
    "        n_estimators: int\n",
    "            Число компонент в композиции\n",
    "\n",
    "        items_rate: float > 0\n",
    "            Размер выборки по которой будет производиться обучение\n",
    "\n",
    "        features_rate: float > 0\n",
    "            Доля фичей, на которой будет обучаться и применяться каждая компонента\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.items_rate = items_rate\n",
    "        self.features_rate = features_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Метод должен обучить композицию, используя X, y как обучающую выборку.\n",
    "        Не забудьте реализовать функционал выбора случайных объектов (с повторениями) и фичей.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array\n",
    "        y: 1d np.array\n",
    "        \"\"\"\n",
    "\n",
    "        # Тут храните обеченные [:)] компоненты\n",
    "        self.estimators = []\n",
    "\n",
    "        # Тут храните фичи для каждой компоненты\n",
    "        self.features_idx = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            # =======================================\n",
    "            # Обучите компоненты смеси\n",
    "            # =======================================\n",
    "            ...\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array матрица объекты-признаки на которых нужно сказать ответ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: 1d np.array, вектор классов для каждого объекта\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = []  # Храните тут ответы каждой компоненты\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # =======================================\n",
    "            # Получите ответы (вероятности) от всех компонент\n",
    "            # ======================================\n",
    "            ...\n",
    "\n",
    "        # =======================================\n",
    "        # Усредните вероятности полученные от этих компонент\n",
    "        # =======================================\n",
    "        y_pred = ...\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "titanic = pd.read_csv('./data/train.csv')[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']]\n",
    "\n",
    "sex_encoder = LabelEncoder()\n",
    "titanic.Sex = sex_encoder.fit_transform(titanic.Sex)\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = titanic[features].values, titanic.Survived.values\n",
    "X = np.nan_to_num(X)\n",
    "X_train, y_train, X_test, y_test = X[:500], y[:500], X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно обучить свой беггинг на датасете титаник, и посмотреть работает ли он. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 10 моделями\n",
    "# =======================================\n",
    "clf = ...\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите эксперименты:\n",
    "    - Работает-ли беггинг лучше чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 100 моделями\n",
    "# =======================================\n",
    "clf = ...\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# Обучите LogsiticRegression \n",
    "# =======================================\n",
    "clf = ...\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы:\n",
    "    - Работает-ли беггинг лучше чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Text, Image Classification</h1> \n",
    "\n",
    "Дальше в каждом эксперименте нужно: \n",
    "- сравниться с линейной моделью ( какую лучше выбрать?=) )\n",
    "- сделать выбор в пользу одной из моделей\n",
    "- выбор обосновать, почему одна из моделей хуже а другая лучше\n",
    "- что такое хуже и лучше\n",
    "- попробуйте беггинг над деревьями и линейными моделями \n",
    "- почему работает или не работает, какие особенности данных на это влияют\n",
    "\n",
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train, y_train = vectorizer.fit_transform(newsgroups_train.data), newsgroups_train.target\n",
    "X_test,  y_test  = vectorizer.transform(newsgroups_test.data), newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# =======================================\n",
    "from sklearn.linear_model import ?\n",
    "\n",
    "clf = ...\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# =======================================\n",
    "\n",
    "clf = ..\n",
    "print(accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import load_cifar10\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10('./data/cifar10')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(X_train.shape[0], -1), X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Bonus part</h1> \n",
    "\n",
    "Это удвоит баллы за дз.\n",
    "\n",
    "- Реализуйте мультиклассовый бустинг -- проверьте на CIFAR10 + SVD\n",
    "- Попробуйте различные функции потерь, придумайте несколько своих, удалось ли обойти логистичискую и экспоненциальную?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
